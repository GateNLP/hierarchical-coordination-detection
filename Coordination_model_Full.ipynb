{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import pandas as pd\n",
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import os\n",
    "from datetime import datetime\n",
    "import math\n",
    "from collections import Counter\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial import distance\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as dist\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entropy function\n",
    "def H(X):\n",
    "    s=sum(X)\n",
    "    h=0\n",
    "    for i in X:\n",
    "        if i!=0:\n",
    "            p=float(i/s)\n",
    "            h=h-(p)*math.log(p)\n",
    "    return h\n",
    "\n",
    "#JSD function\n",
    "def Calculate_JSD(com,user_link,user_link_Entropy):\n",
    "    XX=np.sum([1/len(com)*user_link[i,:] for i in com],axis=0)\n",
    "    JSD=H(XX)\n",
    "    for i in com:\n",
    "        JSD=JSD-((float)(1/len(com))*user_link_Entropy[i])\n",
    "    return JSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine similarity function\n",
    "def Cosine_similarity(user_link1,user_link2):\n",
    "    if norm(user_link1)*norm(user_link2)==0:\n",
    "        return 0\n",
    "    return np.dot(user_link1,user_link2)/(norm(user_link1)*norm(user_link2))\n",
    "\n",
    "\n",
    "#Sort list and count\n",
    "def sort_list(data_key,data_non_key,data_non_key2):\n",
    "    for i in range(len(data_key)):\n",
    "        loc=i\n",
    "        maximum = data_key[i]  \n",
    "        for j in range(i+1,len(data_key)): \n",
    "            if data_key[j] >maximum:\n",
    "                maximum =  data_key[j]\n",
    "                loc=j\n",
    "\n",
    "        tmp=data_key[i]\n",
    "        data_key[i]=data_key[loc]\n",
    "        data_key[loc]=tmp\n",
    "        tmp=data_non_key[i]\n",
    "        data_non_key[i]=data_non_key[loc]\n",
    "        data_non_key[loc]=tmp\n",
    "        tmp2=data_non_key2[i]\n",
    "        data_non_key2[i]=data_non_key2[loc]\n",
    "        data_non_key2[loc]=tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recursivly remove once repeated entitis and users\n",
    "def recursive_remove(Posts):\n",
    "    Flag=True\n",
    "    while (Flag):\n",
    "        len1=len(Posts)\n",
    "        grouped = Posts.groupby('Link')\n",
    "        Once_shared=[]\n",
    "        for group in grouped:\n",
    "            uids=set()\n",
    "            matrix=np.array(group[1][['UserID','Link']].values.tolist())\n",
    "            uids=set(matrix[:,0].tolist())\n",
    "            if len(uids)<=1:\n",
    "                Once_shared.append(matrix[0][1])\n",
    "        Posts =Posts[~Posts[\"Link\"].isin(list(Once_shared))]\n",
    "        Flag=False\n",
    "\n",
    "        len2=len(Posts)\n",
    "        if len1==len2:\n",
    "            Flag=False\n",
    "        print('len1={}\\tlen2={}'.format(len1,len2))\n",
    "\n",
    "        grouped = Posts.groupby('UserID')\n",
    "        shared_one_link=[]\n",
    "        for group in grouped:\n",
    "            lids=set()\n",
    "            matrix=np.array(group[1][['UserID','Link']].values.tolist())\n",
    "            lids=set(matrix[:,1].tolist())\n",
    "            if len(lids)<=1:\n",
    "                shared_one_link.append(matrix[0][0])\n",
    "        Posts =Posts[~Posts[\"UserID\"].isin(list(shared_one_link))]\n",
    "        len3=len(Posts)\n",
    "        if len3==len2:\n",
    "            Flag=False\n",
    "        print('len2={}\\tlen3={}'.format(len2,len3))\n",
    "        \n",
    "    print(\"The number of posts: \", len(Posts))\n",
    "    print(\"The number of unique entities: \", Posts.UserID.nunique())\n",
    "    print(\"The number of unique links: \", Posts.Link.nunique())\n",
    "\n",
    "    return Posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read posts \n",
    "import re\n",
    "#===============================================================================================================================================================================================\n",
    "def extract_hashtags(text: str) -> list[str]:\n",
    "    \"Extracts hashtags from text and returns a list.\"\n",
    "    pattern = r\"(#\\w+)\" #r\"#[^\\s!@#$%^&*()=+./,\\[{\\]};:'\\\"?><]+\"\n",
    "    return re.findall(pattern, text)\n",
    "#Assign Numerical ID\n",
    "def assign_numerical_ID(P,field,ID_field):\n",
    "    unique_set = P[field].unique()\n",
    "    entity_to_id = {entity: id for id, entity in enumerate(unique_set)}\n",
    "    P[ID_field] = P[field].map(entity_to_id)\n",
    "    return P,len(unique_set)\n",
    "#===============================================================================================================================================================================================\n",
    "def read_post(file_name,from_time,to_time):\n",
    "    data = pd.read_csv(file_name, delimiter=',', skipinitialspace=True, engine='python',converters={\"Post_ID\":str, \"Post_text\":str,\"User_ID\":str,\"Post_text\":str})\n",
    "    #data = pd.read_csv(file_name, delimiter=',', skipinitialspace=True, engine='c',converters={\"Post_ID\":str, \"Post_text\":str,\"User_ID\":str,\"Post_text\":str})\n",
    "    data['Post_text'] = data['Post_text'].astype(str)\n",
    "\n",
    "    # Get column indices\n",
    "    text_column = data.columns.get_loc('Post_text')\n",
    "    user_column = data.columns.get_loc('User_ID')\n",
    "    time_column = data.columns.get_loc('Post_time')\n",
    "    post_column = data.columns.get_loc('Post_ID')\n",
    "    is_control_column = data.columns.get_loc('is_control')\n",
    "\n",
    "    hashtag_data = []\n",
    "    for row in data.itertuples(index=False):\n",
    "        text = row[text_column]  # 'Post_text'\n",
    "        hashtags = extract_hashtags(text.lower())\n",
    "\n",
    "        # Ensure unique hashtags\n",
    "        hashtags = set(hashtags)\n",
    "\n",
    "        # Handle missing or problematic Post_time\n",
    "        time_value = row[time_column]\n",
    "\n",
    "        # Check if time_value is not None or NaN before processing\n",
    "        if pd.isna(time_value) or not isinstance(time_value, str):\n",
    "            continue  # Skip this row\n",
    "\n",
    "        try:\n",
    "            timestamp = (time_value.split('+')[0])\n",
    "        except (ValueError, pd.errors.OutOfBoundsDatetime):\n",
    "            continue  # Skip this row if there's an error in datetime conversion\n",
    "\n",
    "        for hashtag in hashtags:\n",
    "            hashtag_data.append({\n",
    "                'UserID': row[user_column],  # 'User_ID'\n",
    "                'Hashtag': hashtag,\n",
    "                'Time':datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S'),\n",
    "                #'Time': timestamp,  # Processed timestamp\n",
    "                'PostID': row[post_column],  # 'Post_ID'\n",
    "                'Is_Control': row[is_control_column]\n",
    "            })\n",
    "\n",
    "    hashtag_df = pd.DataFrame(hashtag_data)\n",
    "    hashtag_df.columns = ['UserID', 'Link', 'PostDate', 'PostID', 'Is_Control']\n",
    "    Posts = hashtag_df[['UserID', 'Link', 'PostDate', 'PostID', 'Is_Control']]\n",
    "\n",
    "    Posts=Posts.loc[(Posts.PostDate>=from_time)&(Posts.PostDate<=to_time)].copy()\n",
    "\n",
    "    Posts=recursive_remove(Posts)\n",
    "    Posts,user_count=assign_numerical_ID(Posts,'UserID','Numeric_UID')\n",
    "    Posts,link_count=assign_numerical_ID(Posts,'Link','Numeric_LID')\n",
    "\n",
    "    return Posts[['UserID', 'Link', 'PostDate','PostID','Is_Control','Numeric_UID','Numeric_LID']], user_count,link_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len1=78962\tlen2=52760\n",
      "len2=52760\tlen3=51607\n",
      "The number of posts:  51607\n",
      "The number of unique entities:  1684\n",
      "The number of unique links:  4634\n"
     ]
    }
   ],
   "source": [
    "#Read the dataset \n",
    "main_path_input =''#input file\n",
    "main_path_output=''#outputfile\n",
    "\n",
    "from_time=datetime.strptime('2017-06-22 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "to_time=datetime.strptime('2018-06-21 23:59:59', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "\n",
    "Posts,user_count,link_count=read_post(main_path_input,from_time,to_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main 1-Multi-edge bipartit graph\n",
    "def Multi_edge_graph(Posts):\n",
    "        \n",
    "    grouped = Posts.groupby('Link')\n",
    "    max_time=0\n",
    "    total_time=0\n",
    "    counter=0\n",
    "    for group in grouped:\n",
    "        matrix=(list(group)[1])[['UserID','PostDate','Link','Numeric_UID','Numeric_LID']].values.tolist()\n",
    "        if len(matrix)>0:\n",
    "            matrix.sort(key= lambda c: (pd.to_datetime(c[1])),reverse=False)   \n",
    "            t0=pd.to_datetime(matrix[0][1])\n",
    "            tn=pd.to_datetime(matrix[len(matrix)-1][1])\n",
    "            timedecay=(tn-t0).total_seconds()\n",
    "            if timedecay>max_time:\n",
    "                max_time=timedecay\n",
    "            total_time+=timedecay\n",
    "            counter+=1\n",
    "    alpha=(math.log(10000)/(total_time/counter))\n",
    "\n",
    "    UserID=[]\n",
    "    Expt=[]\n",
    "    Link=[]\n",
    "    UserNID=[]\n",
    "    LinkNID=[]\n",
    "    for group in grouped:\n",
    "        matrix=(list(group)[1])[['UserID','PostDate','Link','Numeric_UID','Numeric_LID']].values.tolist()\n",
    "        matrix.sort(key= lambda c: (pd.to_datetime(c[1])),reverse=False)   \n",
    "        t0=pd.to_datetime(matrix[0][1])\n",
    "\n",
    "        for i in range(len(matrix)):\n",
    "            ti=pd.to_datetime(matrix[i][1])\n",
    "            td=(ti-t0).total_seconds()\n",
    "            timedecay=math.exp(-alpha*td)\n",
    "            if (timedecay>0.00001):\n",
    "                UserID.append(matrix[i][0])\n",
    "                timedecay=math.exp(-alpha*td)\n",
    "                Expt.append(timedecay)\n",
    "                Link.append(matrix[i][2])\n",
    "                UserNID.append(matrix[i][3])\n",
    "                LinkNID.append(matrix[i][4])\n",
    "\n",
    "    \n",
    "    df=pd.DataFrame()\n",
    "    df['UserID']=UserID\n",
    "    df['TimeDecay']=Expt\n",
    "    df['Link']=Link\n",
    "    df['Numeric_UID']=UserNID\n",
    "    df['Numeric_LID']=LinkNID\n",
    "    return df\n",
    "\n",
    "\n",
    "MEBgraph=Multi_edge_graph(Posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main 2-Convert Muti-edge Bipartite graph into Single_edge_Bipartite graph\n",
    "def Single_edge_graph_summation(Posts):\n",
    "    grouped = Posts.groupby(['UserID','Link'])\n",
    "    UserID=[]\n",
    "    UserNID=[]\n",
    "    Usage=[]\n",
    "    Link=[]\n",
    "    LinkNID=[]\n",
    "    Number_link_used=[]\n",
    "\n",
    "    counter_rep_list=[]\n",
    "    for group in grouped:\n",
    "\n",
    "        matrix=(list(group)[1])[['UserID','TimeDecay','Link','Numeric_UID','Numeric_LID']].values.tolist()\n",
    "        p=0\n",
    "        c=0\n",
    "        if len(matrix)>1:\n",
    "            counter_rep_list.append(len(matrix))  \n",
    "        for i in range(len(matrix)):\n",
    "            p+=matrix[i][1]\n",
    "            c+=1\n",
    "\n",
    "        UserID.append(matrix[i][0])\n",
    "        Usage.append(p)\n",
    "        Link.append(matrix[i][2])\n",
    "        UserNID.append(matrix[i][3])\n",
    "        LinkNID.append(matrix[i][4])\n",
    "        Number_link_used.append(c)\n",
    "    \n",
    "    \n",
    "    df=pd.DataFrame()\n",
    "    df['UserID']=UserID\n",
    "    df['Usage']=Usage\n",
    "    df['Link']=Link\n",
    "    df['Numeric_UID']=UserNID\n",
    "    df['Numeric_LID']=LinkNID\n",
    "    df['Number_link_used']=Number_link_used\n",
    "    return df\n",
    "\n",
    "SEBgraph=Single_edge_graph_summation(MEBgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate initial coordination values\n",
    "def Link_Usage_Behaviour_Matrix3(SEBgraph):\n",
    "    user_behaviour_weight=[]\n",
    "    user_behaviour=[]\n",
    "    user_ID_behaviour=[]\n",
    "    user_behaviour_on_link=[]\n",
    "    min_repeatation=[]\n",
    "    max_repeatition=[]\n",
    "    \n",
    "    grouped = SEBgraph.groupby('Numeric_LID')\n",
    "    for group in grouped:\n",
    "        UserIDs=[]\n",
    "        beha=[]\n",
    "        Number_link_used=[]\n",
    "        matrix=(list(group)[1])[['Numeric_LID','Usage','Number_link_used','Numeric_UID','Link']].values.tolist()\n",
    "\n",
    "        if len(matrix)>1:\n",
    "            for i in range(len(matrix)):\n",
    "                UserIDs.append(matrix[i][3])\n",
    "                beha.append(matrix[i][1])\n",
    "                Number_link_used.append(matrix[i][2])\n",
    "            sort_list(beha,UserIDs,Number_link_used)\n",
    "            max_vector=[]\n",
    "            max_value=-1\n",
    "            for pivot in range(0,len(matrix)):    \n",
    "                h1=H(beha[:pivot]) \n",
    "                h2=H(beha[pivot:])\n",
    "                if h1>h2:\n",
    "                    if h1>max_value:\n",
    "                        max_value=h1\n",
    "                        max_vector=[beha[:pivot],beha[pivot:]]\n",
    "                else:\n",
    "                    if h2>max_value:\n",
    "                        max_value=h2\n",
    "                        max_vector=[beha[:pivot],beha[pivot:]]\n",
    "           \n",
    "            if len(max_vector[0])>0:\n",
    "                vector=max_vector[0]\n",
    "            else:\n",
    "                vector=max_vector[1]\n",
    "\n",
    "\n",
    "            if len(vector)==len(beha):\n",
    "                if abs(H(beha)-H(beha[:-1]))<abs(np.std(beha)-np.std(beha[:-1])):\n",
    "                    vector=vector[:-1]\n",
    "\n",
    "            coordination_size=len(vector)\n",
    "\n",
    "            if coordination_size>1:\n",
    "                user_ID_behaviour.append(UserIDs[:coordination_size])\n",
    "                user_behaviour.append(beha[:coordination_size])\n",
    "                user_behaviour_on_link.append(matrix[0][0])\n",
    "                user_behaviour_weight.append((H(vector)/math.log(coordination_size))*(np.sum(Number_link_used[:coordination_size])/len(Number_link_used[:coordination_size]))*(H(Number_link_used[:coordination_size])/math.log(coordination_size)))\n",
    "                min_repeatation.append(np.min(Number_link_used[:coordination_size]))\n",
    "                max_repeatition.append(np.max(Number_link_used[:coordination_size]))\n",
    "    \n",
    "    \n",
    "    return user_behaviour,user_ID_behaviour,user_behaviour_on_link\n",
    "\n",
    "users_behaviour,edges_users,coordinated_link=Link_Usage_Behaviour_Matrix3(SEBgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DTW function\n",
    "def dp(dist_mat):\n",
    "\n",
    "    N, M = dist_mat.shape\n",
    "    \n",
    "    # Initialize the cost matrix\n",
    "    cost_mat = np.zeros((N + 1, M + 1))\n",
    "    for i in range(1, N + 1):\n",
    "        cost_mat[i, 0] = np.inf\n",
    "    for i in range(1, M + 1):\n",
    "        cost_mat[0, i] = np.inf\n",
    "\n",
    "    # Fill the cost matrix while keeping traceback information\n",
    "    traceback_mat = np.zeros((N, M))\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            penalty = [\n",
    "                cost_mat[i, j],      # match (0)\n",
    "                cost_mat[i, j + 1],  # insertion (1)\n",
    "                cost_mat[i + 1, j]]  # deletion (2)\n",
    "            i_penalty = np.argmin(penalty)\n",
    "            cost_mat[i + 1, j + 1] = dist_mat[i, j] + penalty[i_penalty]\n",
    "            traceback_mat[i, j] = i_penalty\n",
    "\n",
    "    # Traceback from bottom right\n",
    "    i = N - 1\n",
    "    j = M - 1\n",
    "    path = [(i, j)]\n",
    "    while i > 0 or j > 0:\n",
    "        tb_type = traceback_mat[i, j]\n",
    "        if tb_type == 0:\n",
    "            # Match\n",
    "            i = i - 1\n",
    "            j = j - 1\n",
    "        elif tb_type == 1:\n",
    "            # Insertion\n",
    "            i = i - 1\n",
    "        elif tb_type == 2:\n",
    "            # Deletion\n",
    "            j = j - 1\n",
    "        path.append((i, j))\n",
    "\n",
    "    cost_mat = cost_mat[1:, 1:]\n",
    "    return (path[::-1], cost_mat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate edges\n",
    "Edges_form=[]\n",
    "Edges_to=[]\n",
    "beha_1=[]\n",
    "beha_2=[]\n",
    "coor_link=[]\n",
    "for i in range(len(edges_users)):\n",
    "    for j in range(len(edges_users[i])):\n",
    "        for k in range(j+1,len(edges_users[i])):\n",
    "            user1=edges_users[i][j]\n",
    "            user2=edges_users[i][k]\n",
    "            b1=users_behaviour[i][j]\n",
    "            b2=users_behaviour[i][k]\n",
    "            l=coordinated_link[i]\n",
    "\n",
    "            \n",
    "            if user1<user2:\n",
    "                Edges_form.append(user1)\n",
    "                Edges_to.append(user2)\n",
    "                beha_1.append(b1)\n",
    "                beha_2.append(b2)\n",
    "            else:\n",
    "                Edges_form.append(user2)\n",
    "                Edges_to.append(user1)\n",
    "                beha_1.append(b2)\n",
    "                beha_2.append(b1)\n",
    "            coor_link.append(l)\n",
    "\n",
    "\n",
    "\n",
    "df_behaviour=pd.DataFrame()\n",
    "df_behaviour['From']=Edges_form\n",
    "df_behaviour['To']=Edges_to\n",
    "df_behaviour['Beha_1']=beha_1\n",
    "df_behaviour['Beha_2']=beha_2\n",
    "df_behaviour['Numeric_LID']=coor_link\n",
    "del beha_1,beha_2,Edges_form,Edges_to\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunk function\n",
    "def unequal_chunks(list, chunk_size):\n",
    "    chunks = []\n",
    "    b=0\n",
    "    for i in range(0, len(list), chunk_size):\n",
    "        s=b+chunk_size\n",
    "        tb=b+chunk_size+1\n",
    "        while s<len(list) and list[s][0]==list[s-1][0]  and list[s][1]==list[s-1][1]:\n",
    "            s+=1\n",
    "            tb+=1\n",
    "        \n",
    "        if len(list[b:s]>0):\n",
    "            chunks.append(list[b:s])\n",
    "        b=s\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate edges for each chunk\n",
    "\n",
    "\n",
    "def calculated_edges(chunks,df_behaviour,Link_ID):\n",
    "    second_Edges_form=[]\n",
    "    second_Edges_to=[]\n",
    "    second_Edges_weight=[]\n",
    "    second_beha_1=[]\n",
    "    second_beha_2=[]\n",
    "    second_coor_link=[]\n",
    "    \n",
    "\n",
    "    counter=0\n",
    "    TmpFrom= chunks[:,0]\n",
    "    TmpTo= chunks[:,1]\n",
    "    mask=np.zeros(len(df_behaviour),dtype=bool)\n",
    "    preFrom=-1\n",
    "    preTo=-1\n",
    "    MEBgraph_temp=MEBgraph.loc[(MEBgraph.Numeric_UID.isin(chunks[:,0]))|(MEBgraph.Numeric_UID.isin(chunks[:,1]))&(MEBgraph.Numeric_LID.isin(chunks[:,4]))]\n",
    "    for i in range(len(TmpFrom)):\n",
    "        counter+=1\n",
    "        v1=TmpFrom[i]\n",
    "        v2=TmpTo[i]\n",
    "        if v1!=preFrom or v2!=preTo:\n",
    "            preFrom=v1\n",
    "            preTo=v2\n",
    "            \n",
    "            mask=(df_behaviour['From']==v1)&(df_behaviour['To']==v2)    \n",
    "            matrix=df_behaviour[mask][['From','To','Beha_1','Beha_2','Numeric_LID']].to_numpy()\n",
    "            if len(matrix)>1:\n",
    "                ZXZX=[] \n",
    "                for row in matrix:\n",
    "\n",
    "                    X=MEBgraph_temp.loc[(MEBgraph_temp.Numeric_UID==row[0])&(MEBgraph_temp.Numeric_LID==row[4])]['TimeDecay']\n",
    "                    Y=MEBgraph_temp.loc[(MEBgraph_temp.Numeric_UID==row[1])&(MEBgraph_temp.Numeric_LID==row[4])]['TimeDecay']\n",
    "                    x=np.array(sorted(X))\n",
    "                    y=np.array(sorted(Y))\n",
    "\n",
    "                    \n",
    "                    N = x.shape[0]\n",
    "                    M = y.shape[0]\n",
    "                    dist_mat = np.zeros((N, M))\n",
    "                    for i in range(N):\n",
    "                        for j in range(M):\n",
    "                            dist_mat[i, j] = abs(x[i] - y[j])\n",
    "                    path, cost_mat = dp(dist_mat)\n",
    "                    \n",
    "                    \n",
    "                    ZXZX.append(float(1/(1+(cost_mat[N - 1, M - 1]))))\n",
    "                added=np.concatenate([np.abs(matrix[:,2:3]-matrix[:,3:4])],axis=1)\n",
    "                matrix=np.concatenate([matrix,added],axis=1)\n",
    "\n",
    "                added2= np.array(ZXZX)\n",
    "                matrix = np.hstack((matrix, added2[:, np.newaxis]))\n",
    "\n",
    "                sorted_indices=np.argsort(matrix[:,5])\n",
    "                matrix=matrix[sorted_indices]\n",
    "                flag=True\n",
    "                pivot=2\n",
    "\n",
    "                while flag:\n",
    "                    s1=matrix[:pivot,2]\n",
    "                    s2=matrix[:pivot,3]\n",
    "                    d1=matrix[:pivot,5]\n",
    "\n",
    "                    cosine=Cosine_similarity(s1,s2)\n",
    "                    diver=np.sum(d1*d1)\n",
    "                    if diver==0:\n",
    "                        diver=0.00000001\n",
    "                    cosine=cosine/diver\n",
    "                    if cosine<1 or pivot==len(matrix):\n",
    "                        flag=False\n",
    "                        if cosine<1:\n",
    "                            pivot-=1\n",
    "                    else:\n",
    "                        pivot+=1\n",
    "\n",
    "                if(pivot>1):\n",
    "                    u1=np.min([matrix[0,0],matrix[0,1]])\n",
    "                    u2=np.max([matrix[0,0],matrix[0,1]])\n",
    "                        \n",
    "                    for ctr in range(pivot):\n",
    "                        second_Edges_form.append(u1)\n",
    "                        second_Edges_to.append(u2)\n",
    "                        second_beha_1.append(matrix[ctr][2])\n",
    "                        second_beha_2.append(matrix[ctr][3])\n",
    "                        second_coor_link.append(matrix[ctr][4])\n",
    "                        second_Edges_weight.append(matrix[ctr][6])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    del(MEBgraph_temp)\n",
    "    result = np.zeros((len(second_Edges_form), 6)) \n",
    "    result[:,0]=second_Edges_form\n",
    "    result[:,1]=second_Edges_to\n",
    "    result[:,2]=second_beha_1\n",
    "    result[:,3]=second_beha_2\n",
    "    result[:,4]=second_coor_link\n",
    "    result[:,5]=second_Edges_weight\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiprocess determing edges function\n",
    "def multiprocess_edge_calculation(function_reference, file_chunks, arg1,arg2, num_process):\n",
    "    # Create process pool of pre defined size\n",
    "    pool = Pool(num_process)\n",
    "\n",
    "    pbar = tqdm(total=len(file_chunks))\n",
    "\n",
    "    def update(arg):\n",
    "        pbar.update()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(pbar.total):\n",
    "        result=pool.apply_async(function_reference, args=(file_chunks[i],)+ (arg1.loc[(arg1.From.isin(file_chunks[i][:,0]))&(arg1.To.isin(file_chunks[i][:,1]))],arg2,), callback=update)\n",
    "        results.append(result)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    final_result = np.concatenate([r.get() for r in results], axis=0)\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decompress list and call function to run\n",
    "def calculate_edges_with_chunk(dataframe,num_process,chunk_size):\n",
    "    edge_list=dataframe.values\n",
    "    chunks = unequal_chunks(edge_list, chunk_size)\n",
    "    \n",
    "    Link_ID= np.unique(dataframe['Numeric_LID'])\n",
    "    Link_ID.sort()\n",
    "    return multiprocess_edge_calculation(calculated_edges, chunks, dataframe,Link_ID, num_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [-1:59:59<00:00, -17.31it/s]\n"
     ]
    }
   ],
   "source": [
    "#Main4- calculate edges \n",
    "df_behaviour = df_behaviour.sort_values(['From', 'To'])\n",
    "Result=calculate_edges_with_chunk(df_behaviour,8,1000)\n",
    "Result_df=pd.DataFrame({'From':Result[:,0],'To':Result[:,1],'Numeric_LID':Result[:,4],'Weight':Result[:,5]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat user_link probability matrix for every users\n",
    "def user_link_matrix(Posts, user_count, link_count):\n",
    "    user_links = np.zeros((user_count, link_count))\n",
    "    user_links[Posts['Numeric_UID'], Posts['Numeric_LID']] = 1\n",
    "    return user_links\n",
    "\n",
    "user_link_prob=user_link_matrix(Posts,user_count,link_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped=Result_df.groupby(['From','To'])\n",
    "\n",
    "From_ID=[]\n",
    "To_ID=[]\n",
    "Weight=[]\n",
    "\n",
    "\n",
    "for group in grouped:\n",
    "    matrix=(list(group)[1])[['From','To','Numeric_LID','Weight']].values.tolist()\n",
    "    temp=0\n",
    "    for i in range(len(matrix)):\n",
    "        temp+=matrix[i][3]\n",
    "\n",
    "    From_ID.append(matrix[0][0])\n",
    "    To_ID.append(matrix[0][1])    \n",
    "    Weight.append(temp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Thirs_df_behaviour=pd.DataFrame({'Source':From_ID,'Target':To_ID,'Weight':Weight})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "def Entropy1(X):\n",
    "    nonzero_indices = np.nonzero(X)\n",
    "    nonzero_values = X[nonzero_indices]\n",
    "    h = np.sum(-nonzero_values * np.log(nonzero_values))\n",
    "    return h\n",
    "\n",
    "def Entropy2(X):\n",
    "    s=sum(X)\n",
    "    nonzero_indices = np.nonzero(X)\n",
    "    nonzero_values = X[nonzero_indices]\n",
    "    h = np.sum(-nonzero_values/s * np.log(nonzero_values/s))\n",
    "    return h\n",
    "\n",
    "\n",
    "\n",
    "def JSD_Divergance(user_link1,user_link2):\n",
    "    XX=np.zeros(len(user_link1))\n",
    "    s1=sum(user_link1)\n",
    "    s2=sum(user_link2)\n",
    "    XX=0.5*(user_link1/s1+user_link2/s2)\n",
    "    JSD=Entropy1(XX)\n",
    "    JSD=JSD-0.5*Entropy2(user_link1)\n",
    "    JSD=JSD-0.5*Entropy2(user_link2)\n",
    "    return JSD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def divergance_assessment(Thirs_df_behaviour,link_count,user_count,user_link_prob,Result_df):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    From_ID=[]\n",
    "    To_ID=[]\n",
    "    Numeric_LID=[]\n",
    "    Weight=[]\n",
    "    PostIDs_1=[]\n",
    "    PostIDs_2=[]\n",
    "\n",
    "\n",
    "    grouped=Result_df.groupby(['From','To'])\n",
    "\n",
    "    for group in grouped:\n",
    "            \n",
    "        new_weight=1      \n",
    "        matrix=(list(group)[1]).values.tolist()\n",
    "        node_id1=int(matrix[0][0])\n",
    "        node_id2=int(matrix[0][1])\n",
    "        link_IDs = list(np.array(matrix, dtype=object)[:, 2].astype(int))\n",
    "        \n",
    "        max_weight1=np.sum(np.array(matrix, dtype=object)[:, 3]) \n",
    "\n",
    "\n",
    "        Z=np.zeros(link_count)\n",
    "        temp_nodes_ids1 = np.where(np.all(user_link_prob[:, link_IDs], axis=1))[0]\n",
    "\n",
    "        temp_nodes_ids1 = np.delete(temp_nodes_ids1, np.where((temp_nodes_ids1 == node_id1)|(temp_nodes_ids1 == node_id2)))\n",
    "\n",
    "        len_temp_nodes_ids1=len(temp_nodes_ids1) \n",
    "\n",
    "        a0=Thirs_df_behaviour.loc[(Thirs_df_behaviour.Weight.values>=max_weight1)&(((Thirs_df_behaviour.Source.values==node_id1)&(np.isin(Thirs_df_behaviour.Target,temp_nodes_ids1)))|((np.isin(Thirs_df_behaviour.Source,temp_nodes_ids1))&(Thirs_df_behaviour.Target.values==node_id1)))]        \n",
    "        b0=Thirs_df_behaviour.loc[(Thirs_df_behaviour.Weight.values>=max_weight1)&(((Thirs_df_behaviour.Source.values==node_id2)&(np.isin(Thirs_df_behaviour.Target,temp_nodes_ids1)))|((np.isin(Thirs_df_behaviour.Source,temp_nodes_ids1))&(Thirs_df_behaviour.Target.values==node_id2)))]\n",
    "        c0 = pd.concat([a0, b0], ignore_index=True)\n",
    "        Removed_node_IDs=list(set(c0.Source.unique()).union(set(c0.Target.unique())))\n",
    "        temp_nodes_ids1 = np.delete(temp_nodes_ids1, np.where(np.isin(temp_nodes_ids1, Removed_node_IDs)))\n",
    "\n",
    "\n",
    "        a=Thirs_df_behaviour.loc[((Thirs_df_behaviour.Source.values==node_id1)&(np.isin(Thirs_df_behaviour.Target,temp_nodes_ids1)))|((np.isin(Thirs_df_behaviour.Source,temp_nodes_ids1))&(Thirs_df_behaviour.Target.values==node_id1))]\n",
    "        b=Thirs_df_behaviour.loc[((Thirs_df_behaviour.Source.values==node_id2)&(np.isin(Thirs_df_behaviour.Target,temp_nodes_ids1)))|((np.isin(Thirs_df_behaviour.Source,temp_nodes_ids1))&(Thirs_df_behaviour.Target.values==node_id2))]\n",
    "        c = pd.concat([a, b], ignore_index=True)\n",
    "\n",
    "        f1=set(c.Source.unique())\n",
    "        f2=set(c.Target.unique())\n",
    "\n",
    "        temp_nodes_weights_ids1 =list(f1.union(f2))\n",
    "\n",
    "        Weight_vector=np.zeros(user_count)\n",
    "\n",
    "        sum_weight=0\n",
    "        if (len(temp_nodes_ids1)>0):\n",
    "            for ctr2 in temp_nodes_ids1:\n",
    "                X=0\n",
    "                if ctr2 in temp_nodes_weights_ids1:\n",
    "                    c1=c.loc[((np.isin(c.Source,[node_id1,node_id2]))&(c.Target.values==ctr2))|((c.Source.values==ctr2)&(np.isin(c.Target,[node_id1,node_id2])))]\n",
    "                    X=c1['Weight'].max()\n",
    "\n",
    "                X=(max_weight1-X)/max_weight1\n",
    "                Weight_vector[ctr2]=X\n",
    "                sum_weight+=X\n",
    "\n",
    "            User_IDs=set(temp_nodes_ids1)\n",
    "            R_temp=(np.multiply(user_link_prob[i, :],Weight_vector[i]) for i in User_IDs)\n",
    "            Z=np.sum(R_temp, axis=0)\n",
    "            Z=np.divide(Z,sum_weight)\n",
    "            JSD1=JSD_Divergance(user_link_prob[node_id1],Z)\n",
    "            JSD2=JSD_Divergance(user_link_prob[node_id2],Z)\n",
    "            JSD3=JSD_Divergance(user_link_prob[node_id1],user_link_prob[node_id2])     \n",
    "            new_weight=(np.subtract(np.min([JSD1,JSD2]),JSD3))\n",
    "            \n",
    "        for i in range(len(matrix)):\n",
    "            From_ID.append(node_id1)\n",
    "            To_ID.append(node_id2)\n",
    "            Numeric_LID.append(matrix[i][2])\n",
    "            x=matrix[i][3]\n",
    "            if(len_temp_nodes_ids1>0):\n",
    "                divided = sum_weight/len_temp_nodes_ids1\n",
    "                x=x*(1-(divided))+x*new_weight*(divided)\n",
    "            Weight.append(x)\n",
    "\n",
    "            \n",
    "    Result_df=pd.DataFrame({'From':From_ID,'To':To_ID,'Numeric_LID':Numeric_LID,'Weight':Weight})\n",
    "    Result_df=Result_df.loc[Result_df.Weight.values>0]\n",
    "    return Result_df\n",
    "\n",
    "Result_df=divergance_assessment(Thirs_df_behaviour,link_count,user_count,user_link_prob,Result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final results in a CSV file\n",
    "\n",
    "distinct_pairs = Posts[['Link', 'Numeric_LID']].drop_duplicates()\n",
    "merged = pd.merge(Result_df, distinct_pairs, on='Numeric_LID', how='left')\n",
    "merged=merged.drop(columns=['Numeric_LID'])\n",
    "\n",
    "distinct_pairs = Posts[['UserID', 'Numeric_UID']].drop_duplicates()\n",
    "merged = pd.merge(merged, distinct_pairs, left_on='From',right_on='Numeric_UID', how='left')\n",
    "merged=merged.drop(columns=['From','Numeric_UID'])\n",
    "merged=merged.rename(columns={'UserID':'Source'})\n",
    "\n",
    "\n",
    "distinct_pairs = Posts[['UserID', 'Numeric_UID']].drop_duplicates()\n",
    "merged = pd.merge(merged, distinct_pairs, left_on='To',right_on='Numeric_UID', how='left')\n",
    "merged=merged.drop(columns=['To','Numeric_UID'])\n",
    "merged=merged.rename(columns={'UserID':'Target'})\n",
    "merged=merged.rename(columns={'Link':'Hashtag'})\n",
    "\n",
    "grouped_df = merged.groupby(['Source', 'Target'])['Weight'].sum().reset_index()\n",
    "\n",
    "grouped_df.to_csv(main_path_output, encoding='utf-8',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
