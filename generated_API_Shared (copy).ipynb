{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import math\n",
    "from numpy.linalg import norm\n",
    "from multiprocessing import Pool\n",
    "import scipy.spatial.distance as dist\n",
    "import itertools\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Entropy function\n",
    "def calculate_entropy(probabilities):\n",
    "    probabilities=np.array(probabilities)\n",
    "    s=np.sum(probabilities)\n",
    "    if s>0:\n",
    "        probabilities=probabilities/s\n",
    "        probabilities = probabilities[probabilities > 0]  # Exclude zero probabilities\n",
    "        return -np.sum(probabilities * np.log(probabilities))\n",
    "    return 0\n",
    "\n",
    "def update_entropy_incremental(h1, Y, m):\n",
    "    \"\"\"Update entropy after adding value m to list Y.\"\"\"\n",
    "    s=np.sum(Y)\n",
    "    H=0\n",
    "    if s+m>0:\n",
    "        p1 = s /(s+ m)  # New total sum after adding m\n",
    "        p2=  m /(s+ m)\n",
    "        if s==0:\n",
    "            return 0\n",
    "        if p1+p2==0:\n",
    "            H= 0\n",
    "        elif p1==0:\n",
    "            H=-p2*np.log(p2)\n",
    "        elif p2==0:\n",
    "            H=p1*h1-p1*np.log(p1)\n",
    "        else:\n",
    "            H=p1*h1-p1*np.log(p1)-p2*np.log(p2)\n",
    "    return H\n",
    "\n",
    "def Entropy1(X):\n",
    "    nonzero_indices = np.nonzero(X)\n",
    "    nonzero_values = X[nonzero_indices]\n",
    "    h = np.sum(np.multiply(np.negative(nonzero_values),np.log(nonzero_values)))\n",
    "    return h\n",
    "\n",
    "def Entropy2(X,s):\n",
    "    nonzero_indices = np.nonzero(X)\n",
    "    nonzero_values = X[nonzero_indices]\n",
    "    h = np.sum(np.multiply(np.divide(np.negative(nonzero_values),s),np.log(np.divide(nonzero_values,s))))\n",
    "    return h\n",
    "\n",
    "\n",
    "#JSD function\n",
    "def JSD_Divergance(user_link1,user_link2):\n",
    "    # removed this as it didn't seem to be used\n",
    "    s1=np.sum(user_link1)\n",
    "    s2=np.sum(user_link2)\n",
    "    XX=np.multiply(np.add(np.divide(user_link1,s1),np.divide(user_link2,s2)),0.5)    \n",
    "    JSD=Entropy1(XX)\n",
    "    JSD=np.subtract(JSD,np.multiply(Entropy2(user_link1,s1),0.5))\n",
    "    JSD=np.subtract(JSD,np.multiply(Entropy2(user_link2,s2),0.5))\n",
    "    return JSD\n",
    "    \n",
    "# Function to find the maximum entropy partition\n",
    "def find_max_entropy_partition(beha):\n",
    "    max_value = -1\n",
    "    max_partition = None\n",
    "    h1=0\n",
    "    h2=0\n",
    "    E1=[]\n",
    "    E2=[]\n",
    "    Z=beha.copy()\n",
    "    s=np.sum(beha)\n",
    "    if s==0:\n",
    "        return None\n",
    "    beha=beha/s\n",
    "    for pivot in range(len(beha)):\n",
    "        pivot2=len(beha)-pivot\n",
    "        if pivot==0 or pivot==len(beha):\n",
    "            h1=0\n",
    "        else:\n",
    "            \n",
    "            h1 = update_entropy_incremental(h1, beha[:pivot-1], beha[pivot-1])\n",
    "\n",
    "\n",
    "        if pivot2==len(beha):\n",
    "            h2=0\n",
    "        else:\n",
    "            h2 = update_entropy_incremental(h2, beha[pivot2:], beha[pivot2-1])\n",
    "            \n",
    "        E1.append(h1)\n",
    "        E2.append(h2)\n",
    "        \n",
    "    for pivot in range(len(beha)):\n",
    "        h1=E1[pivot]\n",
    "        h2=E2[len(beha)-1-pivot]\n",
    "        if h1 > h2 :\n",
    "            if h1 > max_value:\n",
    "                max_value = h1\n",
    "                max_partition = (beha[:pivot], beha[pivot:])\n",
    "        elif h2 > max_value:\n",
    "            max_value = h2\n",
    "            max_partition = (beha[:pivot], beha[pivot:])\n",
    "\n",
    "    if max_partition:\n",
    "        return max_partition[0] if len(max_partition[0]) > 0 else max_partition[1]\n",
    "    return None\n",
    "\n",
    "#Cosine similarity function\n",
    "def Cosine_similarity(user_link1,user_link2):\n",
    "    if norm(user_link1)*norm(user_link2)==0:\n",
    "        return 0\n",
    "    return np.dot(user_link1,user_link2)/(norm(user_link1)*norm(user_link2))\n",
    "\n",
    "# In[712]:\n",
    "\n",
    "\n",
    "#Assign Numerical ID\n",
    "def assign_numerical_ID(P,field,ID_field):\n",
    "    unique_set = P[field].unique()\n",
    "    entity_to_id = {entity: id for id, entity in enumerate(unique_set)}\n",
    "    P[ID_field] = P[field].map(entity_to_id)\n",
    "    return P,len(unique_set)\n",
    "\n",
    "def extract_hashtags(text: str) -> list[str]:\n",
    "    \"Extracts hashtags from text and returns a list.\"\n",
    "    pattern = r\"(#\\w+)\" #r\"#[^\\s!@#$%^&*()=+./,\\[{\\]};:'\\\"?><]+\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "def extract_urls(text: str) -> list[str]:\n",
    "    \"Extracts urls from text and returns a list.\"\n",
    "    pattern = r\"[(http(s)?):\\/\\/(www\\.)?a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b[-a-zA-Z0-9%_\\+.~#?&//=]*\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "#Read posts\n",
    "def read_post(file_name, ignore_retweets):\n",
    "    data = pd.read_csv(file_name, delimiter=',', skipinitialspace=True, engine='python',converters={\"Post_ID\":str, \"Post_text\":str,\"User_ID\":str,\"Post_text\":str})\n",
    "    data['Post_text']=data['Post_text'].astype(str)\n",
    "    if ignore_retweets:\n",
    "        data = data.drop(data[data['Post_type'] == \"Retweet\"].index)\n",
    "\n",
    "    # rather than using pd.read_csv we just read each line as this also works\n",
    "    # in the case where the user has sent an empty file of excludes\n",
    "    \n",
    "    # these hold the column numbers so a) the columns can move around and\n",
    "    # b) we only have to look them up once\n",
    "    text_column = data.columns.get_loc('Post_text')\n",
    "    user_column = data.columns.get_loc('User_ID')\n",
    "    time_column = data.columns.get_loc('Post_time')\n",
    "    post_column = data.columns.get_loc('Post_ID')\n",
    "\n",
    "    hashtag_data = []\n",
    "    for row in data.itertuples(index=False):\n",
    "        text = row[text_column] # 'Post_text'\n",
    "        hashtags = extract_hashtags(text.lower())\n",
    "        if '#sackdoval' in hashtags:\n",
    "            # make sure to remove any duplicates\n",
    "            hashtags=set(hashtags)\n",
    "            \n",
    "            redate = re.compile('[^0-9]')\n",
    "\n",
    "            for hashtag in hashtags:\n",
    "                    hashtag_data.append({\n",
    "                        'UserID': row[user_column], # 'User_ID'\n",
    "                        'Hashtag': hashtag,\n",
    "                        'Time': pd.Timestamp(row[time_column].split('+')[0]).timestamp(), # 'Post_time'\n",
    "                        'PostID': row[post_column] # 'Post_ID'\n",
    "                    })\n",
    "\n",
    "    hashtag_df = pd.DataFrame(hashtag_data)\n",
    "\n",
    "    hashtag_df.columns = ['UserID', 'Link', 'PostDate', 'PostID']\n",
    "    Posts = hashtag_df.iloc[1:][['UserID', 'Link', 'PostDate','PostID']]\n",
    "    display(Posts.dtypes)\n",
    "    return Posts[['UserID', 'Link', 'PostDate','PostID']], data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[713]:\n",
    "\n",
    "\n",
    "#Recursivly remove once repeated entitis and users\n",
    "def recursive_remove(Posts):\n",
    "    Flag=True\n",
    "    while (Flag):\n",
    "        len1=len(Posts)\n",
    "        grouped = Posts.groupby('Link')\n",
    "        once_shared = set()\n",
    "        for _, group_df in grouped:\n",
    "            user_ids = group_df['UserID'].unique()\n",
    "            if len(user_ids) <= 1:\n",
    "                once_shared.add(group_df['Link'].iloc[0])\n",
    "        Posts = Posts[~np.isin(Posts['Link'],once_shared)]\n",
    "        \n",
    "        len2=len(Posts)\n",
    "        if len1==len2:\n",
    "            Flag=False\n",
    "\n",
    "        grouped = Posts.groupby('UserID')\n",
    "        shared_one_link = set()\n",
    "        for _, group_df in grouped:\n",
    "            links = group_df['Link'].unique()\n",
    "            if len(links) <= 1:\n",
    "                shared_one_link.add(group_df['UserID'].iloc[0])\n",
    "        Posts = Posts[~np.isin(Posts['UserID'],shared_one_link)]\n",
    "\n",
    "        len3=len(Posts)\n",
    "        if len3==len2:\n",
    "            Flag=False\n",
    "        \n",
    "        \n",
    "    return Posts    \n",
    "\n",
    "\n",
    "# In[714]:\n",
    "\n",
    "\n",
    "#filter a percentage of users with toprecords of sharing\n",
    "def filter_top_users(Posts,percentage):\n",
    "    print(\"filtering users\")\n",
    "    user_sharing_counts = Posts['UserID'].value_counts().reset_index()\n",
    "    user_sharing_counts.columns = ['UserID', 'User_sharing_count']\n",
    "    user_sharing_counts.sort_values(by='User_sharing_count', ascending=False, inplace=True)\n",
    "    k = int(len(user_sharing_counts) * (percentage/100))\n",
    "    top_users = user_sharing_counts.head(k)\n",
    "    Posts = Posts[np.isin(Posts['UserID'],top_users['UserID'])]\n",
    "    return Posts\n",
    "\n",
    "# In[715]:\n",
    "\n",
    "\n",
    "#Generate a multi-edge bipartite graph\n",
    "def multi_edge_graph(Posts):\n",
    "    \n",
    "    grouped = Posts.groupby('Link')\n",
    "    \n",
    "    max_time = 0\n",
    "    total_time = 0\n",
    "    counter = 0\n",
    "\n",
    "    for name, group in grouped:\n",
    "        group = group.sort_values(by='PostDate')\n",
    "\n",
    "        t0 = group.iloc[0]['PostDate']\n",
    "        tn = group.iloc[-1]['PostDate']\n",
    "        timedecay = (tn - t0)\n",
    "        #print(type(timedecay),type(max_time))\n",
    "        if timedecay > max_time:\n",
    "            max_time = timedecay\n",
    "\n",
    "        total_time += timedecay\n",
    "        counter += 1\n",
    "\n",
    "    alpha = math.log(10000) / (total_time / counter)\n",
    "    \n",
    "    \n",
    "    UserID = []\n",
    "    Expt = []\n",
    "    Link = []\n",
    "    UserNID = []\n",
    "    LinkNID = []\n",
    "    PostID=[]\n",
    "\n",
    "    for name, group in grouped:\n",
    "        group = group.sort_values(by='PostDate')\n",
    "        t0 = group.iloc[0]['PostDate']\n",
    "        group['TimeDecay'] = group['PostDate'].sub(t0).apply(lambda x: math.exp(-alpha * x))\n",
    "        mask = group['TimeDecay'] > 0.00001\n",
    "        group = group[mask]\n",
    "\n",
    "        UserID.extend(group['UserID'])\n",
    "        Expt.extend(group['TimeDecay'])\n",
    "        Link.extend(group['Link'])\n",
    "        UserNID.extend(group['Numeric_UID'])\n",
    "        LinkNID.extend(group['Numeric_LID'])\n",
    "        PostID.extend(group['PostID'])\n",
    "\n",
    "    data = {\n",
    "        'UserID': UserID,\n",
    "        'TimeDecay': Expt,\n",
    "        'Link': Link,\n",
    "        'Numeric_UID': UserNID,\n",
    "        'Numeric_LID': LinkNID,\n",
    "        'PostID': PostID\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# In[716]:\n",
    "\n",
    "\n",
    "#Convert Muti-edge Bipartite graph into Single_edge_Bipartite graph\n",
    "def single_edge_graph_summation(Posts):\n",
    "    grouped = Posts.groupby(['UserID', 'Link'])\n",
    "    UserID = []\n",
    "    UserNID = []\n",
    "    Usage = []\n",
    "    Link = []\n",
    "    LinkNID = []\n",
    "    Number_link_used = []\n",
    "    PostIDs=[]\n",
    "\n",
    "    for _, group in grouped:\n",
    "        group_size = len(group)\n",
    "        Usage.append(np.sum(group['TimeDecay']))\n",
    "        Number_link_used.append(group_size)\n",
    "        \n",
    "        first_row = group.iloc[0]\n",
    "        UserID.append(first_row['UserID'])\n",
    "        Link.append(first_row['Link'])\n",
    "        UserNID.append(first_row['Numeric_UID'])\n",
    "        LinkNID.append(first_row['Numeric_LID'])\n",
    "        PostIDs.append(group['PostID'].tolist())\n",
    "\n",
    "    data = {\n",
    "        'UserID': UserID,\n",
    "        'Usage': Usage,\n",
    "        'Link': Link,\n",
    "        'Numeric_UID': UserNID,\n",
    "        'Numeric_LID': LinkNID,\n",
    "        'Number_link_used': Number_link_used,\n",
    "        'PostIDs':PostIDs\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# In[717]:\n",
    "\n",
    "\n",
    "#Identify users suspected for coordination basd on individual sharing behaviour\n",
    "def link_usage_behaviour_matrix(SEBgraph):\n",
    "    user_behaviour=[]\n",
    "    user_ID_behaviour=[]\n",
    "    user_behaviour_on_link=[]\n",
    "    PostIDs=[]\n",
    "\n",
    "    \n",
    "    grouped = SEBgraph.groupby('Numeric_LID')\n",
    "    for group in grouped:\n",
    "        group_df = group[1]\n",
    "        numeric_lid = group_df['Numeric_LID'].values[0]\n",
    "        beha = group_df['Usage'].tolist()\n",
    "        user_ids = group_df['Numeric_UID'].tolist()\n",
    "        post_ids=group_df['PostIDs'].tolist()\n",
    "\n",
    "        if len(beha) > 1:\n",
    "            sorted_indices = np.argsort(beha)[::-1]\n",
    "            beha = np.array(beha)[sorted_indices].tolist()\n",
    "            user_ids = np.array(user_ids)[sorted_indices].tolist() \n",
    "            post_ids = np.array(post_ids, dtype=object)[sorted_indices].tolist()\n",
    "            vector = find_max_entropy_partition(beha)\n",
    "\n",
    "            if len(vector)==len(beha):\n",
    "                if abs(calculate_entropy(beha)-calculate_entropy(beha[:-1]))<abs(np.std(beha)-np.std(beha[:-1])):\n",
    "                    vector=vector[:-1]\n",
    "\n",
    "            coordination_size=len(vector)\n",
    " \n",
    "            if coordination_size>1:\n",
    "                user_ID_behaviour.append(user_ids[:coordination_size])\n",
    "                user_behaviour.append(beha[:coordination_size])\n",
    "                user_behaviour_on_link.append(numeric_lid)\n",
    "                PostIDs.append(post_ids[:coordination_size])\n",
    "    \n",
    "    \n",
    "    return user_behaviour,user_ID_behaviour,user_behaviour_on_link,PostIDs\n",
    "\n",
    "# In[718]:\n",
    "\n",
    "\n",
    "#Dynamic Time Wrapping\n",
    "def dtw(dist_mat):\n",
    "    \n",
    "    N, M = dist_mat.shape\n",
    "    \n",
    "    cost_mat = np.zeros((N + 1, M + 1))\n",
    "    for i in range(1, N + 1):\n",
    "        cost_mat[i, 0] = np.inf\n",
    "    for i in range(1, M + 1):\n",
    "        cost_mat[0, i] = np.inf\n",
    "\n",
    "    traceback_mat = np.zeros((N, M))\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            penalty = [\n",
    "                cost_mat[i, j],     \n",
    "                cost_mat[i, j + 1],  \n",
    "                cost_mat[i + 1, j]]  \n",
    "            i_penalty = np.argmin(penalty)\n",
    "            cost_mat[i + 1, j + 1] = dist_mat[i, j] + penalty[i_penalty]\n",
    "            traceback_mat[i, j] = i_penalty\n",
    "\n",
    "    \n",
    "    cost_mat = cost_mat[1:, 1:]\n",
    "    return  cost_mat\n",
    "\n",
    "# In[719]:\n",
    "\n",
    "\n",
    "#Create a dataframe with pairs of users, their behaviour in sharing and the entity\n",
    "def create_edges(users_behaviour, edges_users, coordinated_link,PostIDs):\n",
    "    edges_form, edges_to, beha_1, beha_2,posts_1,posts_2, coor_link = [], [], [], [], [], [], []\n",
    "    for i in range(len(edges_users)):\n",
    "        for j, k in itertools.combinations(range(len(edges_users[i])), 2):\n",
    "            user1, user2 = edges_users[i][j], edges_users[i][k]\n",
    "            b1, b2 = users_behaviour[i][j], users_behaviour[i][k]\n",
    "            p1, p2= PostIDs[i][j], PostIDs[i][k]\n",
    "            l = coordinated_link[i]\n",
    "\n",
    "            if user1 < user2:\n",
    "                edges_form.append(user1)\n",
    "                edges_to.append(user2)\n",
    "                beha_1.append(b1)\n",
    "                beha_2.append(b2)\n",
    "                posts_1.append(p1)\n",
    "                posts_2.append(p2)\n",
    "            else:\n",
    "                edges_form.append(user2)\n",
    "                edges_to.append(user1)\n",
    "                beha_1.append(b2)\n",
    "                beha_2.append(b1)\n",
    "                posts_1.append(p2)\n",
    "                posts_2.append(p1)\n",
    "            coor_link.append(l)\n",
    "\n",
    "    \n",
    "\n",
    "    df_behaviour = pd.DataFrame({\n",
    "        'From': edges_form,\n",
    "        'To': edges_to,\n",
    "        'Beha_1': beha_1,\n",
    "        'Beha_2': beha_2,\n",
    "        'Numeric_LID': coor_link,\n",
    "        'PostIDs_1':posts_1,\n",
    "        'PostIDs_2':posts_2\n",
    "    })\n",
    "    df_behaviour = df_behaviour.sort_values(['From', 'To'])\n",
    "    return df_behaviour\n",
    "\n",
    "# In[720]:\n",
    "\n",
    "\n",
    "#Functions for the parallel process to assess pairwise sharing behaviour\n",
    "\n",
    "#Chunk function\n",
    "def unequal_chunks(list, chunk_size):\n",
    "    chunks = []\n",
    "    b=0\n",
    "    for i in range(0, len(list), chunk_size):\n",
    "        s=b+chunk_size\n",
    "        tb=b+chunk_size+1\n",
    "        while s<len(list) and list[s][0]==list[s-1][0]  and list[s][1]==list[s-1][1]:\n",
    "            s+=1\n",
    "            tb+=1\n",
    "        \n",
    "        if len(list[b:s])>0:\n",
    "            chunks.append(list[b:s])\n",
    "        b=s\n",
    "    return chunks\n",
    "\n",
    "#Function to calculate edges for each chunk\n",
    "def calculated_edges(chunks,df_behaviour,MEBgraph):\n",
    "    second_Edges_form=[]\n",
    "    second_Edges_to=[]\n",
    "    second_Edges_weight=[]\n",
    "    second_beha_1=[]\n",
    "    second_beha_2=[]\n",
    "    second_post_1=[]\n",
    "    second_post_2=[]\n",
    "    second_coor_link=[]\n",
    "    \n",
    "    TmpFrom= chunks[:,0]\n",
    "    TmpTo= chunks[:,1]\n",
    "\n",
    "    chunk_userids_From=set(TmpFrom)\n",
    "    chunk_userids_To=set(TmpTo)\n",
    "    df_behaviour_temp=df_behaviour.loc[(df_behaviour['From'].isin(chunk_userids_From))&(df_behaviour['To'].isin(chunk_userids_To))].copy()\n",
    "\n",
    "    mask=np.zeros(len(df_behaviour_temp),dtype=bool)\n",
    "    preFrom=-1\n",
    "    preTo=-1\n",
    "    MEBgraph_temp=MEBgraph.loc[(MEBgraph.Numeric_UID.isin(chunk_userids_From))|(MEBgraph.Numeric_UID.isin(chunk_userids_To))&(MEBgraph.Numeric_LID.isin(set(chunks[:,4])))].copy()\n",
    "\n",
    "    for i in range(len(TmpFrom)):\n",
    "        v1=TmpFrom[i]\n",
    "        v2=TmpTo[i]\n",
    "        if v1!=preFrom or v2!=preTo:\n",
    "            preFrom=v1\n",
    "            preTo=v2\n",
    "            \n",
    "            mask=(df_behaviour_temp['From']==v1)&(df_behaviour_temp['To']==v2)    \n",
    "            matrix=df_behaviour_temp[mask][['From','To','Beha_1','Beha_2','Numeric_LID','Numeric_LID','PostIDs_1','PostIDs_2']].to_numpy()\n",
    "            if len(matrix)>1:\n",
    "                ZXZX=[] \n",
    "                for row in matrix:\n",
    "\n",
    "                    X=MEBgraph_temp.loc[(MEBgraph_temp.Numeric_UID==row[0])&(MEBgraph_temp.Numeric_LID==row[4])]['TimeDecay']\n",
    "\n",
    "                    Y=MEBgraph_temp.loc[(MEBgraph_temp.Numeric_UID==row[1])&(MEBgraph_temp.Numeric_LID==row[4])]['TimeDecay']\n",
    "\n",
    "                    x=np.array(sorted(X))\n",
    "                    y=np.array(sorted(Y))\n",
    "                    N = x.shape[0]\n",
    "                    M = y.shape[0]\n",
    "\n",
    "                    #with open(\"/home/ahmad/Downloads/Datasets/Coordination_groundtruth/array_shape.txt\", \"a\") as file:\n",
    "                    #    file.write(f\"Shape of the array: {len(matrix)}  {row[0]}  {row[4]} {row[1]} {X} {Y} {N}    {M}  \\n\")\n",
    "                    if N>1000 and M>1000:\n",
    "                        ZXZX.append(1)\n",
    "                    else:\n",
    "                        dist_mat = np.zeros((N, M))\n",
    "                        for i in range(N):\n",
    "                            for j in range(M):\n",
    "                                dist_mat[i, j] = abs(x[i] - y[j])\n",
    "                        cost_mat = dtw(dist_mat)\n",
    "                        ZXZX.append(float(1/(1+(cost_mat[N - 1, M - 1]))))\n",
    "                added=np.concatenate([np.abs(matrix[:,2:3]-matrix[:,3:4])],axis=1)\n",
    "                matrix=np.concatenate([matrix,added],axis=1)\n",
    "\n",
    "                added2= np.array(ZXZX)\n",
    "                matrix = np.hstack((matrix, added2[:, np.newaxis]))\n",
    "\n",
    "                sorted_indices=np.argsort(matrix[:,8])\n",
    "                matrix=matrix[sorted_indices]\n",
    "                flag=True\n",
    "                pivot=2\n",
    "\n",
    "                while flag:\n",
    "                    s1=matrix[:pivot,2]\n",
    "                    s2=matrix[:pivot,3]\n",
    "                    d1=matrix[:pivot,8]\n",
    "\n",
    "                    cosine=Cosine_similarity(s1,s2)\n",
    "                    diver=np.sum(np.multiply(d1,d1))\n",
    "                    if diver==0:\n",
    "                        diver=0.00000001\n",
    "                    cosine=cosine/diver\n",
    "                    if cosine<1 or pivot==len(matrix):\n",
    "                        flag=False\n",
    "                        if cosine<1:\n",
    "                            pivot-=1\n",
    "                    else:\n",
    "                        pivot+=1\n",
    "\n",
    "                if(pivot>1):\n",
    "                    u1=np.min([matrix[0,0],matrix[0,1]])\n",
    "                    u2=np.max([matrix[0,0],matrix[0,1]])\n",
    "                        \n",
    "                    for ctr in range(pivot):\n",
    "                        second_Edges_form.append(u1)\n",
    "                        second_Edges_to.append(u2)\n",
    "                        second_beha_1.append(matrix[ctr][2])\n",
    "                        second_beha_2.append(matrix[ctr][3])\n",
    "                        second_coor_link.append(matrix[ctr][4])\n",
    "                        second_post_1.append(matrix[ctr][6])\n",
    "                        second_post_2.append(matrix[ctr][7])\n",
    "                        second_Edges_weight.append(matrix[ctr][9])\n",
    "    \n",
    "    # Create a structured array to hold the lists\n",
    "    result = np.empty(shape=(len(second_Edges_form),), dtype=[\n",
    "        ('int_col1', int),\n",
    "        ('int_col2', int),\n",
    "        ('int_col3', int),\n",
    "        ('float_col', float),\n",
    "        ('list_col1', list),\n",
    "        ('list_col2', list)\n",
    "    ])\n",
    "\n",
    "    # Assign values to the structured array\n",
    "    result['int_col1'] = second_Edges_form\n",
    "    result['int_col2'] = second_Edges_to\n",
    "    result['int_col3'] = second_coor_link\n",
    "    result['float_col'] = second_Edges_weight\n",
    "    result['list_col1'] = second_post_1\n",
    "    result['list_col2'] = second_post_2\n",
    "\n",
    "    #result = np.empty(shape=(len(second_Edges_form), 6), dtype=[('',int),('',int),('',int),('',float),('',object),('',object)])\n",
    "    #print(second_post_1,type(second_post_1))\n",
    "    #result = np.empty((len(second_Edges_form), 6)) \n",
    "    #result[:,0]=second_Edges_form\n",
    "    #result[:,1]=second_Edges_to\n",
    "    #result[:,2]=second_coor_link\n",
    "    #result[:,3]=second_Edges_weight\n",
    "    #for i in range(len(second_Edges_form)):\n",
    "    #    result[i,4]=second_post_1[i]\n",
    "    #    result[i,5]=second_post_2[i]\n",
    "    del(MEBgraph_temp)\n",
    "    \n",
    "    \n",
    "    return result\n",
    "\n",
    "#Multiprocess determing edges function\n",
    "def multiprocess_edge_calculation(function_reference, file_chunks, arg1,arg2, num_process):\n",
    "    pool = Pool(num_process)\n",
    "    pbar = tqdm(total=len(file_chunks))\n",
    "    def update(arg):\n",
    "        pbar.update()\n",
    "    results = []\n",
    "    for i in range(pbar.total):\n",
    "        result=pool.apply_async(function_reference, args=(file_chunks[i],)+ (arg1,arg2,),callback=update)      \n",
    "        results.append(result)\n",
    "        \n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    final_result =np.concatenate([r.get() for r in results], axis=0)\n",
    "    return final_result\n",
    "\n",
    "#Decompress list and call function to run\n",
    "def calculate_edges_with_chunk(dataframe,MEBgraph,num_process,chunk_size):\n",
    "    edge_list=dataframe.values\n",
    "    chunks = unequal_chunks(edge_list, chunk_size)\n",
    "    return multiprocess_edge_calculation(calculated_edges, chunks, dataframe,MEBgraph, num_process)\n",
    "\n",
    "\n",
    "# In[721]:\n",
    "\n",
    "\n",
    "#Creat user_link probability matrix for every users\n",
    "def user_link_matrix(Posts, user_count, link_count):\n",
    "    user_links = np.zeros((user_count, link_count))\n",
    "    user_links[Posts['Numeric_UID'], Posts['Numeric_LID']] = 1\n",
    "    return user_links\n",
    "\n",
    "\n",
    "#calculate weight of edges between pairs of users (sum of the coordination weight for all entities)\n",
    "def calualte_edge_weight(df):\n",
    "    grouped = df.groupby(['From', 'To'])\n",
    "\n",
    "    From_ID = []\n",
    "    To_ID = []\n",
    "    Weight = []\n",
    "\n",
    "    for group, group_df in grouped:\n",
    "        temp = np.sum(group_df['Weight'])\n",
    "        From_ID.append(group[0])\n",
    "        To_ID.append(group[1])\n",
    "        Weight.append(temp)\n",
    "\n",
    "    df_out = pd.DataFrame({'Source': From_ID, 'Target': To_ID, 'Weight': Weight})\n",
    "    return df_out\n",
    "\n",
    "\n",
    "#Assess the similarity of non-coordinated users and suspicious users\n",
    "def divergance_assessment(Thirs_df_behaviour,link_count,user_count,user_link_prob,Result_df):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    From_ID=[]\n",
    "    To_ID=[]\n",
    "    Numeric_LID=[]\n",
    "    Weight=[]\n",
    "    PostIDs_1=[]\n",
    "    PostIDs_2=[]\n",
    "\n",
    "\n",
    "    grouped=Result_df.groupby(['From','To'])\n",
    "\n",
    "    for group in grouped:\n",
    "            \n",
    "        new_weight=1      \n",
    "        matrix=(list(group)[1]).values.tolist()\n",
    "        node_id1=int(matrix[0][0])\n",
    "        node_id2=int(matrix[0][1])\n",
    "        link_IDs = list(np.array(matrix, dtype=object)[:, 2].astype(int))\n",
    "        \n",
    "        max_weight1=np.sum(np.array(matrix, dtype=object)[:, 3]) \n",
    "\n",
    "\n",
    "        Z=np.zeros(link_count)\n",
    "        temp_nodes_ids1 = np.where(np.all(user_link_prob[:, link_IDs], axis=1))[0]\n",
    "\n",
    "        temp_nodes_ids1 = np.delete(temp_nodes_ids1, np.where((temp_nodes_ids1 == node_id1)|(temp_nodes_ids1 == node_id2)))\n",
    "\n",
    "        len_temp_nodes_ids1=len(temp_nodes_ids1) \n",
    "\n",
    "        a0=Thirs_df_behaviour.loc[(Thirs_df_behaviour.Weight.values>=max_weight1)&(((Thirs_df_behaviour.Source.values==node_id1)&(np.isin(Thirs_df_behaviour.Target,temp_nodes_ids1)))|((np.isin(Thirs_df_behaviour.Source,temp_nodes_ids1))&(Thirs_df_behaviour.Target.values==node_id1)))]        \n",
    "        b0=Thirs_df_behaviour.loc[(Thirs_df_behaviour.Weight.values>=max_weight1)&(((Thirs_df_behaviour.Source.values==node_id2)&(np.isin(Thirs_df_behaviour.Target,temp_nodes_ids1)))|((np.isin(Thirs_df_behaviour.Source,temp_nodes_ids1))&(Thirs_df_behaviour.Target.values==node_id2)))]\n",
    "        c0 = pd.concat([a0, b0], ignore_index=True)\n",
    "        Removed_node_IDs=list(set(c0.Source.unique()).union(set(c0.Target.unique())))\n",
    "        temp_nodes_ids1 = np.delete(temp_nodes_ids1, np.where(np.isin(temp_nodes_ids1, Removed_node_IDs)))\n",
    "\n",
    "\n",
    "        a=Thirs_df_behaviour.loc[((Thirs_df_behaviour.Source.values==node_id1)&(np.isin(Thirs_df_behaviour.Target,temp_nodes_ids1)))|((np.isin(Thirs_df_behaviour.Source,temp_nodes_ids1))&(Thirs_df_behaviour.Target.values==node_id1))]\n",
    "        b=Thirs_df_behaviour.loc[((Thirs_df_behaviour.Source.values==node_id2)&(np.isin(Thirs_df_behaviour.Target,temp_nodes_ids1)))|((np.isin(Thirs_df_behaviour.Source,temp_nodes_ids1))&(Thirs_df_behaviour.Target.values==node_id2))]\n",
    "        c = pd.concat([a, b], ignore_index=True)\n",
    "\n",
    "        f1=set(c.Source.unique())\n",
    "        f2=set(c.Target.unique())\n",
    "\n",
    "        temp_nodes_weights_ids1 =list(f1.union(f2))\n",
    "\n",
    "        Weight_vector=np.zeros(user_count)\n",
    "\n",
    "        sum_weight=0\n",
    "        if (len(temp_nodes_ids1)>0):\n",
    "            for ctr2 in temp_nodes_ids1:\n",
    "                X=0\n",
    "                if ctr2 in temp_nodes_weights_ids1:\n",
    "                    c1=c.loc[((np.isin(c.Source,[node_id1,node_id2]))&(c.Target.values==ctr2))|((c.Source.values==ctr2)&(np.isin(c.Target,[node_id1,node_id2])))]\n",
    "                    X=c1['Weight'].max()\n",
    "\n",
    "                X=(max_weight1-X)/max_weight1\n",
    "                Weight_vector[ctr2]=X\n",
    "                sum_weight+=X\n",
    "\n",
    "            User_IDs=set(temp_nodes_ids1)\n",
    "            R_temp=(np.multiply(user_link_prob[i, :],Weight_vector[i]) for i in User_IDs)\n",
    "            Z=np.sum(R_temp, axis=0)\n",
    "            Z=np.divide(Z,sum_weight)\n",
    "            JSD1=JSD_Divergance(user_link_prob[node_id1],Z)\n",
    "            JSD2=JSD_Divergance(user_link_prob[node_id2],Z)\n",
    "            JSD3=JSD_Divergance(user_link_prob[node_id1],user_link_prob[node_id2])     \n",
    "            new_weight=(np.subtract(np.min([JSD1,JSD2]),JSD3))\n",
    "            \n",
    "        for i in range(len(matrix)):\n",
    "            From_ID.append(node_id1)\n",
    "            To_ID.append(node_id2)\n",
    "            Numeric_LID.append(matrix[i][2])\n",
    "            x=matrix[i][3]\n",
    "            if(len_temp_nodes_ids1>0):\n",
    "                divided = sum_weight/len_temp_nodes_ids1\n",
    "                x=x*(1-(divided))+x*new_weight*(divided)\n",
    "            Weight.append(x)\n",
    "            PostIDs_1.append(matrix[i][4])\n",
    "            PostIDs_2.append(matrix[i][5])\n",
    "            \n",
    "    Result_df=pd.DataFrame({'From':From_ID,'To':To_ID,'Numeric_LID':Numeric_LID,'Weight':Weight,'PostIDs_from':PostIDs_1,'PostIDs_to':PostIDs_2})\n",
    "    Result_df=Result_df.loc[Result_df.Weight.values>0]\n",
    "    return Result_df\n",
    "\n",
    "    \n",
    "\n",
    "# In[722]:\n",
    "\n",
    "\n",
    "#Convert numerical_Ids to original Ids and write the final results in outputfile\n",
    "def generate_result(Result_df, Posts):\n",
    "    distinct_pairs = Posts[['Link', 'Numeric_LID']].drop_duplicates()\n",
    "    merged = pd.merge(Result_df, distinct_pairs, on='Numeric_LID', how='left')\n",
    "    merged=merged.drop(columns=['Numeric_LID'])\n",
    "\n",
    "    distinct_pairs = Posts[['UserID', 'Numeric_UID']].drop_duplicates()\n",
    "    merged = pd.merge(merged, distinct_pairs, left_on='From',right_on='Numeric_UID', how='left')\n",
    "    merged=merged.drop(columns=['From','Numeric_UID'])\n",
    "    merged=merged.rename(columns={'UserID':'From'})\n",
    "\n",
    "    \n",
    "    distinct_pairs = Posts[['UserID', 'Numeric_UID']].drop_duplicates()\n",
    "    merged = pd.merge(merged, distinct_pairs, left_on='To',right_on='Numeric_UID', how='left')\n",
    "    merged=merged.drop(columns=['To','Numeric_UID'])\n",
    "    merged=merged.rename(columns={'UserID':'To'})\n",
    "    \n",
    "    new_order = ['From', 'To', 'Link', 'Weight','PostIDs_from','PostIDs_to']  # Adjust the column names accordingly    \n",
    "    merged = merged[new_order]\n",
    "\n",
    "    return merged.sort_values(by=['Weight'], ascending=False) \n",
    "    \n",
    "def generate_json_result(df, raw_data):\n",
    "\n",
    "    node_names= (pd.concat([df['From'],df['To']])).unique()\n",
    "    \n",
    "    \n",
    "    post_text = raw_data.groupby([\"User_ID\"]).agg(screen_name=('Screen_Name',\"first\"), posts=('Post_ID', list)).reset_index()\n",
    "    post_text = post_text[np.isin(post_text['User_ID'],node_names)]\n",
    "\n",
    "    nodes = []\n",
    "    edges = []\n",
    "\n",
    "    for row in post_text.itertuples(index=False):\n",
    "        # 0 = UserID, 1 = screen_name, 2 = posts\n",
    "        nodes.append(dict(key=str(row[0]), attributes=dict(label=str(row[1]), posts=row[2])))\n",
    "\n",
    "    combined = pd.pivot_table(df, index=[\"From\",\"To\"], values=[\"Weight\",\"Link\",\"PostIDs_from\",\"PostIDs_to\"], aggfunc= {\"Weight\": [\"sum\",list],\"Link\":list,\"PostIDs_from\":list,\"PostIDs_to\":list})\n",
    "\n",
    "    #combined[\"id\"] = combined.index\n",
    "\n",
    "    combined = combined.sort_values(by=(\"Weight\",\"sum\"), ascending=False)\n",
    "\n",
    "    for row in combined.itertuples():\n",
    "        # 0 = the index tuple, 5 = Weight/sum, 1 = Link/list, 4 = Weight/list, 2 = PostIDs_from, 3 = PostIDs_to\n",
    "        edges.append(dict(source=str(row[0][0]),target=str(row[0][1]), attributes=dict(size=row[5]*1, hashtags=row[1], weights=row[4], source=row[2], target=row[3])))\n",
    "\n",
    "    return dict(nodes=nodes,edges=edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incremental_option():\n",
    "    print(\"Is it an incremental running of the model:\")\n",
    "    print(\"1. No\")\n",
    "    print(\"2. Yes\")\n",
    "\n",
    "\n",
    "    # Ask the user for input\n",
    "    while True:\n",
    "        choice = input(\"Enter the number of your choice (1 or 2): \").strip()\n",
    "        if choice in {'1', '2'}:\n",
    "            return int(choice)\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter 1 or 2.\")\n",
    "\n",
    "\n",
    "def get_speed_option():\n",
    "    print(\"Please select one of the following options for speed:\")\n",
    "    print(\"1. Fast\")\n",
    "    print(\"2. Moderate\")\n",
    "    print(\"3. Slow\")\n",
    "\n",
    "    # Ask the user for input\n",
    "    while True:\n",
    "        choice = input(\"Enter the number of your choice (1, 2, or 3): \").strip()\n",
    "        if choice in {'1', '2', '3'}:\n",
    "            return int(choice)\n",
    "        else:\n",
    "            print(\"Invalid input. Please enter 1, 2, or 3.\")\n",
    "\n",
    "def accomulate_results(result,path_output):\n",
    "    latest_update=pd.read_csv(path_output)\n",
    "    merged_results = pd.concat([result, latest_update])\n",
    "\n",
    "    merged_results = merged_results.groupby(['From', 'To','Link'], as_index=False)['Weight'].max()\n",
    "\n",
    "    return merged_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it an incremental running of the model:\n",
      "1. No\n",
      "2. Yes\n",
      "Please select one of the following options for speed:\n",
      "1. Fast\n",
      "2. Moderate\n",
      "3. Slow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UserID       object\n",
       "Link         object\n",
       "PostDate    float64\n",
       "PostID       object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "#def calcCoordination(jobId):\n",
    "\n",
    "\n",
    "path_input = '/home/ahmad/Downloads/input_isis.csv'\n",
    "\n",
    "incremental_option = get_incremental_option()\n",
    "speed_option = get_speed_option()\n",
    "\n",
    "# What percentage of users with the highest number of shared entities should be included?\n",
    "# (a number between 1 to 100)\n",
    "top_percent = 100\n",
    "# How many threads should be used for multi-threaded execution?\n",
    "num_process = int(os.environ.get(\"COORDINATION_WORKER_THREADS\", \"8\"))\n",
    "# How many chunks should each thread consider?\n",
    "chunk_size = int(os.environ.get(\"COORDINATION_WORKER_CHUNK_SIZE\", \"1000\"))\n",
    "\n",
    "Posts,raw_data=read_post(path_input,\"1\") # options[0], including retweets or not\n",
    "# don't filter if we actually want everyone anyway\n",
    "if top_percent != 100:\n",
    "    Posts=filter_top_users(Posts,top_percent)\n",
    "\n",
    "Posts=recursive_remove(Posts)\n",
    "Posts,user_count=assign_numerical_ID(Posts,'UserID','Numeric_UID')\n",
    "Posts,link_count=assign_numerical_ID(Posts,'Link','Numeric_LID')\n",
    "\n",
    "if len(Posts) == 0:\n",
    "    print( \"no overlap in sharing behaviour\")\n",
    "\n",
    "\n",
    "\n",
    "#Create a multi-edge bipartite graph\n",
    "MEBgraph=multi_edge_graph(Posts)\n",
    "#display(MEBgraph)\n",
    "#Convert the multi-edge graph to a single-edge bipartite graph\n",
    "SEBgraph=single_edge_graph_summation(MEBgraph)\n",
    "#For each hashtag: identifying users sharing suspiciously and edeg between them  (individual level)\n",
    "users_behaviour,edges_users,coordinated_link,PostIDs=link_usage_behaviour_matrix(SEBgraph)\n",
    "df_behaviour=create_edges(users_behaviour, edges_users, coordinated_link,PostIDs)\n",
    "if speed_option==1:\n",
    "    Final_coordination_records=df_behaviour.copy()\n",
    "\n",
    "    pair_counts = Final_coordination_records.groupby(['From', 'To']).size()\n",
    "    more_once__pairs = pair_counts[pair_counts > 1].index\n",
    "    Final_coordination_records = Final_coordination_records[Final_coordination_records.set_index(['From', 'To']).index.isin(more_once__pairs)]\n",
    "\n",
    "    Final_coordination_records['Weight']=Final_coordination_records[['Beha_1', 'Beha_2']].min(axis=1)\n",
    "    Final_coordination_records['PostIDs_from']= Final_coordination_records['PostIDs_1'].apply(lambda x: x[0] if len(x) > 0 else None)\n",
    "    Final_coordination_records['PostIDs_to']= Final_coordination_records['PostIDs_2'].apply(lambda x: x[0] if len(x) > 0 else None)\n",
    "    Final_coordination_records=Final_coordination_records[['From','To','Numeric_LID','Weight','PostIDs_from','PostIDs_to']]\n",
    "    \n",
    "    result = generate_result(Final_coordination_records, Posts)\n",
    "    if incremental_option==2:\n",
    "        result=accomulate_results(result,'path_output')    \n",
    "    \n",
    "else:\n",
    "    #Identifying users sharing suspiciously (pairwise level)\n",
    "    edges=calculate_edges_with_chunk(df_behaviour,MEBgraph,num_process,chunk_size)\n",
    "    column_names = ['From', 'To', 'Numeric_LID', 'Weight','PostIDs_from','PostIDs_to']\n",
    "    pairwis_coordination = pd.DataFrame(edges)#{col: edges[:, idx] for idx, col in enumerate(columns)})\n",
    "    pairwis_coordination.columns=column_names\n",
    "    \n",
    "    \n",
    "    if speed_option==2:\n",
    "        Final_coordination_records=pairwis_coordination.copy()\n",
    "\n",
    "        Final_coordination_records['PostIDs_from']= Final_coordination_records['PostIDs_from'].apply(lambda x: x[0] if len(x) > 0 else None)\n",
    "        Final_coordination_records['PostIDs_to']= Final_coordination_records['PostIDs_to'].apply(lambda x: x[0] if len(x) > 0 else None)\n",
    "        Final_coordination_records=Final_coordination_records[['From','To','Numeric_LID','Weight','PostIDs_from','PostIDs_to']]\n",
    "\n",
    "    \n",
    "        result = generate_result(Final_coordination_records, Posts)\n",
    "\n",
    "    else:\n",
    "        #Identifying users sharing suspiciously (group level)\n",
    "        user_link_prob = user_link_matrix(Posts, user_count, link_count)\n",
    "        group_coordination=calualte_edge_weight(pairwis_coordination)\n",
    "        Final_coordination_records=divergance_assessment(group_coordination,link_count,user_count,user_link_prob,pairwis_coordination)\n",
    "\n",
    "        Final_coordination_records['PostIDs_from']= Final_coordination_records['PostIDs_from'].apply(lambda x: x[0] if len(x) > 0 else None)\n",
    "        Final_coordination_records['PostIDs_to']= Final_coordination_records['PostIDs_to'].apply(lambda x: x[0] if len(x) > 0 else None)\n",
    "        Final_coordination_records=Final_coordination_records[['From','To','Numeric_LID','Weight','PostIDs_from','PostIDs_to']]\n",
    "        \n",
    "        result = generate_result(Final_coordination_records, Posts)\n",
    "        \n",
    "\n",
    "\n",
    "result = generate_result(Final_coordination_records, Posts)\n",
    "\n",
    "\n",
    "#return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Link</th>\n",
       "      <th>Weight</th>\n",
       "      <th>PostIDs_from</th>\n",
       "      <th>PostIDs_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aabekosar</td>\n",
       "      <td>saleha_khan22</td>\n",
       "      <td>#sackdoval</td>\n",
       "      <td>0.121619</td>\n",
       "      <td>752502145606361089</td>\n",
       "      <td>752504933686149120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aabekosar</td>\n",
       "      <td>FarahVaseem</td>\n",
       "      <td>#isis</td>\n",
       "      <td>0.125101</td>\n",
       "      <td>752504585957306368</td>\n",
       "      <td>752503173382897664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>AyishaBaloch</td>\n",
       "      <td>saleha_khan22</td>\n",
       "      <td>#isis</td>\n",
       "      <td>0.128372</td>\n",
       "      <td>752503469597159424</td>\n",
       "      <td>752504941995057157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aabekosar</td>\n",
       "      <td>FarahVaseem</td>\n",
       "      <td>#sackdoval</td>\n",
       "      <td>0.134626</td>\n",
       "      <td>752502145606361089</td>\n",
       "      <td>752502943761461248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>KhatijahFatima</td>\n",
       "      <td>sadiayousaf14</td>\n",
       "      <td>#isis</td>\n",
       "      <td>0.152170</td>\n",
       "      <td>752508896544251904</td>\n",
       "      <td>752505817631514625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>afia_jameel</td>\n",
       "      <td>taniasyed5</td>\n",
       "      <td>#iraq</td>\n",
       "      <td>0.918231</td>\n",
       "      <td>752522317629124609</td>\n",
       "      <td>752524152259698688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>BadalSays</td>\n",
       "      <td>RebuildPAK</td>\n",
       "      <td>#un</td>\n",
       "      <td>0.940276</td>\n",
       "      <td>752532366997028864</td>\n",
       "      <td>752527820035858432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>BadalSays</td>\n",
       "      <td>RebuildPAK</td>\n",
       "      <td>#dhaka</td>\n",
       "      <td>0.988011</td>\n",
       "      <td>752512696130691072</td>\n",
       "      <td>752512876452032512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>shahidnazir4821</td>\n",
       "      <td>BadalSays</td>\n",
       "      <td>#sackdoval</td>\n",
       "      <td>0.990880</td>\n",
       "      <td>752510848116125696</td>\n",
       "      <td>752512696130691072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>BadalSays</td>\n",
       "      <td>RebuildPAK</td>\n",
       "      <td>#sackdoval</td>\n",
       "      <td>0.998341</td>\n",
       "      <td>752512696130691072</td>\n",
       "      <td>752511889628860419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                From             To        Link    Weight        PostIDs_from  \\\n",
       "3          Aabekosar  saleha_khan22  #sackdoval  0.121619  752502145606361089   \n",
       "1          Aabekosar    FarahVaseem       #isis  0.125101  752504585957306368   \n",
       "44      AyishaBaloch  saleha_khan22       #isis  0.128372  752503469597159424   \n",
       "0          Aabekosar    FarahVaseem  #sackdoval  0.134626  752502145606361089   \n",
       "53    KhatijahFatima  sadiayousaf14       #isis  0.152170  752508896544251904   \n",
       "..               ...            ...         ...       ...                 ...   \n",
       "205      afia_jameel     taniasyed5       #iraq  0.918231  752522317629124609   \n",
       "237        BadalSays     RebuildPAK         #un  0.940276  752532366997028864   \n",
       "236        BadalSays     RebuildPAK      #dhaka  0.988011  752512696130691072   \n",
       "30   shahidnazir4821      BadalSays  #sackdoval  0.990880  752510848116125696   \n",
       "235        BadalSays     RebuildPAK  #sackdoval  0.998341  752512696130691072   \n",
       "\n",
       "             PostIDs_to  \n",
       "3    752504933686149120  \n",
       "1    752503173382897664  \n",
       "44   752504941995057157  \n",
       "0    752502943761461248  \n",
       "53   752505817631514625  \n",
       "..                  ...  \n",
       "205  752524152259698688  \n",
       "237  752527820035858432  \n",
       "236  752512876452032512  \n",
       "30   752512696130691072  \n",
       "235  752511889628860419  \n",
       "\n",
       "[238 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values(by='Weight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
